{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Application of Deep Reinforcement Learning to Algorithmic Trading\n",
    "\n",
    "By Thibaut Théatea, Damien Ernsta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run datadownloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "startingDate = \"2012-01-01\"\n",
    "start_validation_date = \"2016-01-01\"\n",
    "splitting_date = '2018-01-01'\n",
    "endingDate = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>12.375389</td>\n",
       "      <td>12.413608</td>\n",
       "      <td>12.308281</td>\n",
       "      <td>12.320319</td>\n",
       "      <td>302220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>12.441895</td>\n",
       "      <td>12.479212</td>\n",
       "      <td>12.316706</td>\n",
       "      <td>12.338374</td>\n",
       "      <td>260022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>12.580027</td>\n",
       "      <td>12.595675</td>\n",
       "      <td>12.418724</td>\n",
       "      <td>12.487339</td>\n",
       "      <td>271269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-06</th>\n",
       "      <td>12.711534</td>\n",
       "      <td>12.722066</td>\n",
       "      <td>12.615836</td>\n",
       "      <td>12.632388</td>\n",
       "      <td>318292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-09</th>\n",
       "      <td>12.691375</td>\n",
       "      <td>12.872538</td>\n",
       "      <td>12.679939</td>\n",
       "      <td>12.804828</td>\n",
       "      <td>394024400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Close       High        Low       Open     Volume\n",
       "Date                                                             \n",
       "2012-01-03  12.375389  12.413608  12.308281  12.320319  302220800\n",
       "2012-01-04  12.441895  12.479212  12.316706  12.338374  260022000\n",
       "2012-01-05  12.580027  12.595675  12.418724  12.487339  271269600\n",
       "2012-01-06  12.711534  12.722066  12.615836  12.632388  318292800\n",
       "2012-01-09  12.691375  12.872538  12.679939  12.804828  394024400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_training = pd.read_csv('data/AAPL_'+startingDate+'_'+splitting_date+'.csv').set_index('Date')\n",
    "aapl_validation = pd.read_csv('data/AAPL_'+start_validation_date+'_'+splitting_date+'.csv').set_index('Date')\n",
    "aapl_test = pd.read_csv('data/AAPL_'+splitting_date+'_'+endingDate+'.csv').set_index('Date')\n",
    "aapl_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation(data, initialSpace, observationSpace):\n",
    "    df = data.copy()\n",
    "    return data.iloc[initialSpace - observationSpace:initialSpace]\n",
    "\n",
    "def update_observation(observation, action, initialSpace):\n",
    "    observation['Action'].iloc[initialSpace] = action\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TQDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:44:31.001893: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters related to the Experience Replay mechanism\n",
    "capacity = 100000\n",
    "batchSize = 32\n",
    "experiencesRequired = 1000\n",
    "\n",
    "###############################################################################\n",
    "############################### Class ReplayMemory ############################\n",
    "###############################################################################\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    GOAL: Implementing the replay memory required for the Experience Replay\n",
    "          mechanism of the DQN Reinforcement Learning algorithm.\n",
    "    \n",
    "    VARIABLES:  - memory: Data structure storing the experiences.\n",
    "                                \n",
    "    METHODS:    - __init__: Initialization of the memory data structure.\n",
    "                - push: Insert a new experience into the replay memory.\n",
    "                - sample: Sample a batch of experiences from the replay memory.\n",
    "                - __len__: Return the length of the replay memory.\n",
    "                - reset: Reset the replay memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=capacity):\n",
    "        \"\"\"\n",
    "        GOAL: Initializating the replay memory data structure.\n",
    "        \n",
    "        INPUTS: - capacity: Capacity of the data structure, specifying the\n",
    "                            maximum number of experiences to be stored\n",
    "                            simultaneously.\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        memory = deque(maxlen=capacity)\n",
    "    \n",
    "\n",
    "    def push(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        GOAL: Insert a new experience into the replay memory. An experience\n",
    "              is composed of a state, an action, a reward, a next state and\n",
    "              a termination signal.\n",
    "        \n",
    "        INPUTS: - state: RL state of the experience to be stored.\n",
    "                - action: RL action of the experience to be stored.\n",
    "                - reward: RL reward of the experience to be stored.\n",
    "                - nextState: RL next state of the experience to be stored.\n",
    "                - done: RL termination signal of the experience to be stored.\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        memory.append((state, action, reward, nextState, done))\n",
    "\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        \"\"\"\n",
    "        GOAL: Sample a batch of experiences from the replay memory.\n",
    "        \n",
    "        INPUTS: - batchSize: Size of the batch to sample.\n",
    "        \n",
    "        OUTPUTS: - state: RL states of the experience batch sampled.\n",
    "                 - action: RL actions of the experience batch sampled.\n",
    "                 - reward: RL rewards of the experience batch sampled.\n",
    "                 - nextState: RL next states of the experience batch sampled.\n",
    "                 - done: RL termination signals of the experience batch sampled.\n",
    "        \"\"\"\n",
    "\n",
    "        state, action, reward, nextState, done = zip(*random.sample(memory, batchSize))\n",
    "        return state, action, reward, nextState, done\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        GOAL: Return the capicity of the replay memory, which is the maximum number of\n",
    "              experiences which can be simultaneously stored in the replay memory.\n",
    "        \n",
    "        INPUTS: /\n",
    "        \n",
    "        OUTPUTS: - length: Capacity of the replay memory.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(memory)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        GOAL: Reset (empty) the replay memory.\n",
    "        \n",
    "        INPUTS: /\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        memory = deque(maxlen=capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters related to the DQN algorithm\n",
    "gamma = 0.4\n",
    "learningRate = 0.0001\n",
    "targetNetworkUpdate = 1000\n",
    "learningUpdatePeriod = 1\n",
    "\n",
    "# Default parameters related to the Deep Neural Network\n",
    "numberOfNeurons = 512\n",
    "dropout = 0.2\n",
    "\n",
    "###############################################################################\n",
    "################################### Class DQN #################################\n",
    "###############################################################################\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    GOAL: Implementing the Deep Neural Network of the DQN Reinforcement \n",
    "          Learning algorithm.\n",
    "    \n",
    "    VARIABLES:  - fc1: Fully Connected layer number 1.\n",
    "                - fc2: Fully Connected layer number 2.\n",
    "                - fc3: Fully Connected layer number 3.\n",
    "                - fc4: Fully Connected layer number 4.\n",
    "                - fc5: Fully Connected layer number 5.\n",
    "                - dropout1: Dropout layer number 1.\n",
    "                - dropout2: Dropout layer number 2.\n",
    "                - dropout3: Dropout layer number 3.\n",
    "                - dropout4: Dropout layer number 4.\n",
    "                - bn1: Batch normalization layer number 1.\n",
    "                - bn2: Batch normalization layer number 2.\n",
    "                - bn3: Batch normalization layer number 3.\n",
    "                - bn4: Batch normalization layer number 4.\n",
    "                                \n",
    "    METHODS:    - __init__: Initialization of the Deep Neural Network.\n",
    "                - forward: Forward pass of the Deep Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numberOfInputs, numberOfOutputs, numberOfNeurons=numberOfNeurons, dropout=dropout):\n",
    "        \"\"\"\n",
    "        GOAL: Defining and initializing the Deep Neural Network of the\n",
    "              DQN Reinforcement Learning algorithm.\n",
    "        \n",
    "        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n",
    "                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n",
    "                - numberOfNeurons: Number of neurons per layer in the Deep Neural Network.\n",
    "                - dropout: Droupout probability value (handling of overfitting).\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the constructor of the parent class (Pytorch torch.nn.Module)\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Definition of some Fully Connected layers\n",
    "        fc1 = nn.Linear(numberOfInputs, numberOfNeurons, bias=False)\n",
    "        fc2 = nn.Linear(numberOfNeurons, numberOfNeurons, bias=False)\n",
    "        fc3 = nn.Linear(numberOfNeurons, numberOfNeurons, bias=False)\n",
    "        fc4 = nn.Linear(numberOfNeurons, numberOfNeurons, bias=False)\n",
    "        fc5 = nn.Linear(numberOfNeurons, numberOfOutputs)\n",
    "\n",
    "        # Definition of some Batch Normalization layers\n",
    "        bn1 = nn.BatchNorm1d(numberOfNeurons)\n",
    "        bn2 = nn.BatchNorm1d(numberOfNeurons)\n",
    "        bn3 = nn.BatchNorm1d(numberOfNeurons)\n",
    "        bn4 = nn.BatchNorm1d(numberOfNeurons)\n",
    "\n",
    "        # Definition of some Dropout layers.\n",
    "        dropout1 = nn.Dropout(dropout)\n",
    "        dropout2 = nn.Dropout(dropout)\n",
    "        dropout3 = nn.Dropout(dropout)\n",
    "        dropout4 = nn.Dropout(dropout)\n",
    "\n",
    "        # Xavier initialization for the entire neural network\n",
    "        torch.nn.init.xavier_uniform_(fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(fc3.weight)\n",
    "        torch.nn.init.xavier_uniform_(fc4.weight)\n",
    "        torch.nn.init.xavier_uniform_(fc5.weight)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        GOAL: Implementing the forward pass of the Deep Neural Network.\n",
    "        \n",
    "        INPUTS: - input: Input of the Deep Neural Network.\n",
    "        \n",
    "        OUTPUTS: - output: Output of the Deep Neural Network.\n",
    "        \"\"\"\n",
    "\n",
    "        x = dropout1(F.leaky_relu(bn1(fc1(input))))\n",
    "        x = dropout2(F.leaky_relu(bn2(fc2(x))))\n",
    "        x = dropout3(F.leaky_relu(bn3(fc3(x))))\n",
    "        x = dropout4(F.leaky_relu(bn4(fc4(x))))\n",
    "        output = fc5(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(numberOfInputs, numberOfOutputs, numberOfNeurons=numberOfNeurons, dropout=dropout):\n",
    "    \"\"\"\n",
    "    GOAL: Implement a Deep Q-Network (DQN) using a functional approach.\n",
    "    \n",
    "    INPUTS:\n",
    "    - numberOfInputs: Number of input features.\n",
    "    - numberOfOutputs: Number of possible actions (Q-values output).\n",
    "    - numberOfNeurons: Number of neurons per hidden layer (default: 128).\n",
    "    - dropout: Dropout probability for regularization (default: 0.1).\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - A PyTorch Sequential model representing the Deep Q-Network.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(numberOfInputs, numberOfNeurons, biais=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "\n",
    "        nn.Linear(numberOfNeurons, numberOfNeurons, biais=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "\n",
    "        nn.Linear(numberOfNeurons, numberOfNeurons, biais=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "\n",
    "        nn.Linear(numberOfNeurons, numberOfNeurons, biais=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        \n",
    "        nn.Linear(numberOfNeurons, numberOfOutputs)\n",
    "    )\n",
    "    \n",
    "    # Initialize weights using Xavier initialization\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables defining the default observation and state spaces\n",
    "stateLength = 30\n",
    "observationSpace = 1 + (stateLength-1)*4\n",
    "actionSpace = 2\n",
    "\n",
    "# Default parameter related to the L2 Regularization \n",
    "L2Factor = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizationCoefficients(tradingEnv):\n",
    "    \"\"\"\n",
    "    GOAL: Retrieve the coefficients required for the normalization\n",
    "            of input data.\n",
    "    \n",
    "    INPUTS: - tradingEnv: RL trading environement to process.\n",
    "    \n",
    "    OUTPUTS: - coefficients: Normalization coefficients.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the available trading data\n",
    "    tradingData = tradingEnv.data\n",
    "    closePrices = tradingData['Close'].tolist()\n",
    "    lowPrices = tradingData['Low'].tolist()\n",
    "    highPrices = tradingData['High'].tolist()\n",
    "    volumes = tradingData['Volume'].tolist()\n",
    "\n",
    "    # Retrieve the coefficients required for the normalization\n",
    "    coefficients = []\n",
    "    margin = 1\n",
    "    # 1. Close price => returns (absolute) => maximum value (absolute)\n",
    "    returns = [abs((closePrices[i]-closePrices[i-1])/closePrices[i-1]) for i in range(1, len(closePrices))]\n",
    "    coeffs = (0, np.max(returns)*margin)\n",
    "    coefficients.append(coeffs)\n",
    "    # 2. Low/High prices => Delta prices => maximum value\n",
    "    deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(len(lowPrices))]\n",
    "    coeffs = (0, np.max(deltaPrice)*margin)\n",
    "    coefficients.append(coeffs)\n",
    "    # 3. Close/Low/High prices => Close price position => no normalization required\n",
    "    coeffs = (0, 1)\n",
    "    coefficients.append(coeffs)\n",
    "    # 4. Volumes => minimum and maximum values\n",
    "    coeffs = (np.min(volumes)/margin, np.max(volumes)*margin)\n",
    "    coefficients.append(coeffs)\n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(state, coefficients):\n",
    "    \"\"\"\n",
    "    GOAL: Process the RL state returned by the environment\n",
    "            (appropriate format and normalization).\n",
    "    \n",
    "    INPUTS: - state: RL state returned by the environment.\n",
    "    \n",
    "    OUTPUTS: - state: Processed RL state.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalization of the RL state\n",
    "    closePrices = [state[0][i] for i in range(len(state[0]))]\n",
    "    lowPrices = [state[1][i] for i in range(len(state[1]))]\n",
    "    highPrices = [state[2][i] for i in range(len(state[2]))]\n",
    "    volumes = [state[3][i] for i in range(len(state[3]))]\n",
    "\n",
    "    # 1. Close price => returns => MinMax normalization\n",
    "    returns = [(closePrices[i]-closePrices[i-1])/closePrices[i-1] for i in range(1, len(closePrices))]\n",
    "    if coefficients[0][0] != coefficients[0][1]:\n",
    "        state[0] = [((x - coefficients[0][0])/(coefficients[0][1] - coefficients[0][0])) for x in returns]\n",
    "    else:\n",
    "        state[0] = [0 for x in returns]\n",
    "    # 2. Low/High prices => Delta prices => MinMax normalization\n",
    "    deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(1, len(lowPrices))]\n",
    "    if coefficients[1][0] != coefficients[1][1]:\n",
    "        state[1] = [((x - coefficients[1][0])/(coefficients[1][1] - coefficients[1][0])) for x in deltaPrice]\n",
    "    else:\n",
    "        state[1] = [0 for x in deltaPrice]\n",
    "    # 3. Close/Low/High prices => Close price position => No normalization required\n",
    "    closePricePosition = []\n",
    "    for i in range(1, len(closePrices)):\n",
    "        deltaPrice = abs(highPrices[i]-lowPrices[i])\n",
    "        if deltaPrice != 0:\n",
    "            item = abs(closePrices[i]-lowPrices[i])/deltaPrice\n",
    "        else:\n",
    "            item = 0.5\n",
    "        closePricePosition.append(item)\n",
    "    if coefficients[2][0] != coefficients[2][1]:\n",
    "        state[2] = [((x - coefficients[2][0])/(coefficients[2][1] - coefficients[2][0])) for x in closePricePosition]\n",
    "    else:\n",
    "        state[2] = [0.5 for x in closePricePosition]\n",
    "    # 4. Volumes => MinMax normalization\n",
    "    volumes = [volumes[i] for i in range(1, len(volumes))]\n",
    "    if coefficients[3][0] != coefficients[3][1]:\n",
    "        state[3] = [((x - coefficients[3][0])/(coefficients[3][1] - coefficients[3][0])) for x in volumes]\n",
    "    else:\n",
    "        state[3] = [0 for x in volumes]\n",
    "    \n",
    "    # Process the state structure to obtain the appropriate format\n",
    "    state = [item for sublist in state for item in sublist]\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(state, mainNetwork):\n",
    "        \"\"\"\n",
    "        GOAL: Choose a valid RL action from the action space according to the\n",
    "              RL policy as well as the current RL state observed.\n",
    "        \n",
    "        INPUTS: - state: RL state returned by the environment.\n",
    "        \n",
    "        OUTPUTS: - action: RL action chosen from the action space.\n",
    "                 - Q: State-action value function associated.\n",
    "                 - QValues: Array of all the Qvalues outputted by the\n",
    "                            Deep Neural Network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Choose the best action based on the RL policy\n",
    "        with torch.no_grad():\n",
    "            tensorState = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            QValues = mainNetwork(tensorState).squeeze(0)\n",
    "            Q, action = QValues.max(0)\n",
    "            action = action.item()\n",
    "            Q = Q.item()\n",
    "            QValues = QValues.cpu().numpy()\n",
    "            return action, Q, QValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters related to the Epsilon-Greedy exploration technique\n",
    "epsilonStart = 1.0\n",
    "epsilonEnd = 0.01\n",
    "epsilonDecay = 10000\n",
    "\n",
    "# Default parameters regarding the sticky actions RL generalization technique\n",
    "alpha = 0.1\n",
    "\n",
    "def chooseActionEpsilonGreedy(state, previousAction, mainNetwork, iterations):\n",
    "    \"\"\"\n",
    "    GOAL: Choose a valid RL action from the action space according to the\n",
    "            RL policy as well as the current RL state observed, following the \n",
    "            Epsilon Greedy exploration mechanism.\n",
    "\n",
    "    INPUTS: - state: RL state returned by the environment.\n",
    "            - previousAction: Previous RL action executed by the agent.\n",
    "\n",
    "    OUTPUTS: - action: RL action chosen from the action space.\n",
    "                - Q: State-action value function associated.\n",
    "                - QValues: Array of all the Qvalues outputted by the\n",
    "                        Deep Neural Network.\n",
    "    \"\"\"\n",
    "    epsilonValue = lambda iteration: epsilonEnd + (epsilonStart - epsilonEnd) * math.exp(-1 * iteration / epsilonDecay)\n",
    "    # EXPLOITATION -> RL policy\n",
    "    if(random.random() > epsilonValue(iterations)):\n",
    "        # Sticky action (RL generalization mechanism)\n",
    "        if(random.random() > alpha):\n",
    "            action, Q, QValues = chooseAction(state, mainNetwork)\n",
    "        else:\n",
    "            action = previousAction\n",
    "            Q = 0\n",
    "            QValues = [0, 0]\n",
    "\n",
    "    # EXPLORATION -> Random\n",
    "    else:\n",
    "        action = random.randrange(actionSpace)\n",
    "        Q = 0\n",
    "        QValues = [0, 0]\n",
    "\n",
    "    # Increment the iterations counter (for Epsilon Greedy)\n",
    "    iterations += 1\n",
    "\n",
    "    return action, Q, QValues, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLowerBound(cash, numberOfShares, price, transactionCosts, epsilon):\n",
    "    \"\"\"\n",
    "    GOAL: Compute the lower bound of the complete RL action space, \n",
    "            i.e. the minimum number of share to trade.\n",
    "    \n",
    "    INPUTS: - cash: Value of the cash owned by the agent.\n",
    "            - numberOfShares: Number of shares owned by the agent.\n",
    "            - price: Last price observed.\n",
    "    \n",
    "    OUTPUTS: - lowerBound: Lower bound of the RL action space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Computation of the RL action lower bound\n",
    "    deltaValues = - cash - numberOfShares * price * (1 + epsilon) * (1 + transactionCosts)\n",
    "    if deltaValues < 0:\n",
    "        lowerBound = deltaValues / (price * (2 * transactionCosts + (epsilon * (1 + transactionCosts))))\n",
    "    else:\n",
    "        lowerBound = deltaValues / (price * epsilon * (1 + transactionCosts))\n",
    "    return lowerBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(action, data, t, numberOfShares, transactionCosts, stateLength, epsilon):\n",
    "    \"\"\"\n",
    "    GOAL: Transition to the next trading time step based on the\n",
    "            trading position decision made (either long or short).\n",
    "    \n",
    "    INPUTS: - action: Trading decision (1 = long, 0 = short).    \n",
    "    \n",
    "    OUTPUTS: - state: RL state to be returned to the RL agent.\n",
    "                - reward: RL reward to be returned to the RL agent.\n",
    "                - done: RL episode termination signal (boolean).\n",
    "                - info: Additional information returned to the RL agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stting of some local variables\n",
    "    t = t\n",
    "    numberOfShares = numberOfShares\n",
    "    customReward = False\n",
    "\n",
    "    # CASE 1: LONG POSITION\n",
    "    if(action == 1):\n",
    "        data['Position'][t] = 1\n",
    "        # Case a: Long -> Long\n",
    "        if(data['Position'][t - 1] == 1):\n",
    "            data['Cash'][t] = data['Cash'][t - 1]\n",
    "            data['Holdings'][t] = numberOfShares * data['Close'][t]\n",
    "        # Case b: No position -> Long\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            data['Holdings'][t] = numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = 1\n",
    "        # Case c: Short -> Long\n",
    "        else:\n",
    "            data['Cash'][t] = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            numberOfShares = math.floor(data['Cash'][t]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            data['Holdings'][t] = numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = 1\n",
    "\n",
    "    # CASE 2: SHORT POSITION\n",
    "    elif(action == 0):\n",
    "        data['Position'][t] = -1\n",
    "        # Case a: Short -> Short\n",
    "        if(data['Position'][t - 1] == -1):\n",
    "            lowerBound = computeLowerBound(data['Cash'][t - 1], -numberOfShares, data['Close'][t-1], transactionCosts, epsilon)\n",
    "            if lowerBound <= 0:\n",
    "                data['Cash'][t] = data['Cash'][t - 1]\n",
    "                data['Holdings'][t] =  - numberOfShares * data['Close'][t]\n",
    "            else:\n",
    "                numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n",
    "                numberOfShares -= numberOfSharesToBuy\n",
    "                data['Cash'][t] = data['Cash'][t - 1] - numberOfSharesToBuy * data['Close'][t] * (1 + transactionCosts)\n",
    "                data['Holdings'][t] =  - numberOfShares * data['Close'][t]\n",
    "                customReward = True\n",
    "        # Case b: No position -> Short\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            data['Holdings'][t] = - numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = -1\n",
    "        # Case c: Long -> Short\n",
    "        else:\n",
    "            data['Cash'][t] = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            numberOfShares = math.floor(data['Cash'][t]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            data['Holdings'][t] = - numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = -1\n",
    "\n",
    "    # CASE 3: PROHIBITED ACTION\n",
    "    else:\n",
    "        raise SystemExit(\"Prohibited action! Action should be either 1 (long) or 0 (short).\")\n",
    "\n",
    "    # Update the total amount of money owned by the agent, as well as the return generated\n",
    "    data['Money'][t] = data['Holdings'][t] + data['Cash'][t]\n",
    "    data['Returns'][t] = (data['Money'][t] - data['Money'][t-1])/data['Money'][t-1]\n",
    "\n",
    "    # Set the RL reward returned to the trading agent\n",
    "    if not customReward:\n",
    "        reward = data['Returns'][t]\n",
    "    else:\n",
    "        reward = (data['Close'][t-1] - data['Close'][t])/data['Close'][t-1]\n",
    "\n",
    "    # Transition to the next trading time step\n",
    "    t = t + 1\n",
    "    state = [data['Close'][t - stateLength : t].tolist(),\n",
    "                    data['Low'][t - stateLength : t].tolist(),\n",
    "                    data['High'][t - stateLength : t].tolist(),\n",
    "                    data['Volume'][t - stateLength : t].tolist(),\n",
    "                    [data['Position'][t - 1]]]\n",
    "    if(t == data.shape[0]):\n",
    "        done = 1  \n",
    "\n",
    "    # Same reasoning with the other action (exploration trick)\n",
    "    otherAction = int(not bool(action))\n",
    "    customReward = False\n",
    "    if(otherAction == 1):\n",
    "        otherPosition = 1\n",
    "        if(data['Position'][t - 1] == 1):\n",
    "            otherCash = data['Cash'][t - 1]\n",
    "            otherHoldings = numberOfShares * data['Close'][t]\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            otherHoldings = numberOfShares * data['Close'][t]\n",
    "        else:\n",
    "            otherCash = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            numberOfShares = math.floor(otherCash/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = otherCash - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            otherHoldings = numberOfShares * data['Close'][t]\n",
    "    else:\n",
    "        otherPosition = -1\n",
    "        if(data['Position'][t - 1] == -1):\n",
    "            lowerBound = computeLowerBound(data['Cash'][t - 1], -numberOfShares, data['Close'][t-1], transactionCosts, epsilon)\n",
    "            if lowerBound <= 0:\n",
    "                otherCash = data['Cash'][t - 1]\n",
    "                otherHoldings =  - numberOfShares * data['Close'][t]\n",
    "            else:\n",
    "                numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n",
    "                numberOfShares -= numberOfSharesToBuy\n",
    "                otherCash = data['Cash'][t - 1] - numberOfSharesToBuy * data['Close'][t] * (1 + transactionCosts)\n",
    "                otherHoldings =  - numberOfShares * data['Close'][t]\n",
    "                customReward = True\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            otherHoldings = - numberOfShares * data['Close'][t]\n",
    "        else:\n",
    "            otherCash = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            numberOfShares = math.floor(otherCash/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = otherCash + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            otherHoldings = - numberOfShares * data['Close'][t]\n",
    "    otherMoney = otherHoldings + otherCash\n",
    "    if not customReward:\n",
    "        otherReward = (otherMoney - data['Money'][t-1])/data['Money'][t-1]\n",
    "    else:\n",
    "        otherReward = (data['Close'][t-1] - data['Close'][t])/data['Close'][t-1]\n",
    "    otherState = [data['Close'][t - stateLength : t].tolist(),\n",
    "                    data['Low'][t - stateLength : t].tolist(),\n",
    "                    data['High'][t - stateLength : t].tolist(),\n",
    "                    data['Volume'][t - stateLength : t].tolist(),\n",
    "                    [otherPosition]]\n",
    "    info = {'State' : otherState, 'Reward' : otherReward, 'Done' : done}\n",
    "\n",
    "    # Return the trading environment feedback to the RL trading agent\n",
    "    return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQDN\n",
    "\n",
    "# 1. Initialize - Set the two Deep Neural Networks of the DQN algorithm (policy/main and target)\n",
    "replay_memory = ReplayMemory(capacity=capacity)  # Experience replay memory\n",
    "main_network = DQN(observationSpace, actionSpace, numberOfNeurons, dropout)   # Main DQN (θ), with Xavier init\n",
    "target_network = DQN(observationSpace, actionSpace, numberOfNeurons, dropout) # Target DQN (θ⁻)\n",
    "target_network.load_state_dict(main_network.state_dict())  # Initialize target network with main network\n",
    "main_network.eval()\n",
    "target_network.eval()\n",
    "\n",
    "# Set the Deep Learning optimizer\n",
    "optimizer = optim.Adam(main_network.parameters(), lr=learningRate, weight_decay=L2Factor)  # Adam optimizer\n",
    "\n",
    "# Set the Epsilon-Greedy exploration technique\n",
    "epsilonValue = lambda iteration: epsilonEnd + (epsilonStart - epsilonEnd) * math.exp(-1 * iteration / epsilonDecay)\n",
    "\n",
    "# Initialization of the iterations counter\n",
    "iterations = 0\n",
    "\n",
    "# 2. Training Loop\n",
    "for episode in range(1, N + 1):\n",
    "    \n",
    "    # Set the initial RL variables\n",
    "    coefficients = getNormalizationCoefficients(aapl_training)\n",
    "    state = processState(aapl_training, coefficients)\n",
    "    previousAction = 0\n",
    "\n",
    "    for t in range(1, T + 1):\n",
    "\n",
    "        # 3. Epsilon-Greedy Policy\n",
    "        if random() < epsilon:\n",
    "            action, _, _, iterations  = chooseActionEpsilonGreedy(state, previousAction, main_network, iterations)  # random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = main_network(torch.FloatTensor(observation))\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "        # 4. Environment Duplication Trick\n",
    "        env_copy = copy.deepcopy(env)\n",
    "\n",
    "        # 5. Step with action and anti-action\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        anti_action = get_opposite_action(action)\n",
    "        next_obs_anti, reward_anti, _ = env_copy.step(anti_action)\n",
    "\n",
    "        next_obs = preprocess(next_obs)\n",
    "        next_obs_anti = preprocess(next_obs_anti)\n",
    "\n",
    "        # 6. Store both experiences\n",
    "        replay_memory.add((observation, action, reward, next_obs))\n",
    "        replay_memory.add((observation, anti_action, reward_anti, next_obs_anti))\n",
    "\n",
    "        observation = next_obs\n",
    "\n",
    "        # 7. Training Step (every T’ steps)\n",
    "        if t % T_prime == 0:\n",
    "            minibatch = replay_memory.sample(batch_size)\n",
    "\n",
    "            states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "            states = torch.FloatTensor(states)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "            rewards = torch.FloatTensor(rewards)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "            # Compute target values yi using Double DQN trick\n",
    "            with torch.no_grad():\n",
    "                next_actions = main_network(next_states).argmax(dim=1, keepdim=True)\n",
    "                next_q_values = target_network(next_states).gather(1, next_actions)\n",
    "                targets = rewards + gamma * next_q_values.squeeze()\n",
    "\n",
    "            # Compute Q-values\n",
    "            q_values = main_network(states).gather(1, actions).squeeze()\n",
    "\n",
    "            # Compute and clip gradients using Huber loss\n",
    "            loss = F.smooth_l1_loss(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(main_network.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Soft update of target network\n",
    "            if t % N_target_update == 0:\n",
    "                target_network.load_state_dict(main_network.state_dict())\n",
    "\n",
    "        # 8. Update epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TQDN(observationSpace, actionSpace, numberOfNeurons=numberOfNeurons, dropout=dropout, \n",
    "        gamma=gamma, learningRate=learningRate, targetNetworkUpdate=targetNetworkUpdate,\n",
    "        epsilonStart=epsilonStart, epsilonEnd=epsilonEnd, epsilonDecay=epsilonDecay,\n",
    "        capacity=capacity, batchSize=batchSize):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
