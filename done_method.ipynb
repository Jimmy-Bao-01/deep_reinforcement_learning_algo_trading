{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a478fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a2b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 18:49:56.994272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92159bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = {\n",
    "    'Dow Jones' : 'DIA',\n",
    "    'S&P 500' : 'SPY',\n",
    "    'NASDAQ 100' : 'QQQ',\n",
    "    'FTSE 100' : 'EZU',\n",
    "    'Nikkei 225' : 'EWJ',\n",
    "    'Google' : 'GOOGL',\n",
    "    'Apple' : 'AAPL',\n",
    "    'Meta' : 'META',\n",
    "    'Amazon' : 'AMZN',\n",
    "    'Microsoft' : 'MSFT',\n",
    "    'Nokia' : 'NOK',\n",
    "    'Philips' : 'PHIA.AS',\n",
    "    'Siemens' : 'SIE.DE',\n",
    "    'Baidu' : 'BIDU',\n",
    "    'Alibaba' : 'BABA',\n",
    "    'Tencent' : '0700.HK',\n",
    "    'Sony' : '6758.T',\n",
    "    'JPMorgan Chase' : 'JPM',\n",
    "    'HSBC' : 'HSBC',\n",
    "    'CCB' : '0939.HK',\n",
    "    'ExxonMobil' : 'XOM',\n",
    "    'Shell' : 'SHEL',\n",
    "    'PetroChina' : '0857.HK',\n",
    "    'Tesla' : 'TSLA',\n",
    "    'Volkswagen' : 'VOW3.DE',\n",
    "    'Toyota' : '7203.T',\n",
    "    'Coca Cola' : 'KO',\n",
    "    'AB InBev' : 'ABI.BR',\n",
    "    'Kirin' : '2503.T',\n",
    "    # 'Twitter' : 'TWTR' # No accessible now\n",
    "}\n",
    "\n",
    "# Variables defining the default trading horizon\n",
    "start_date = \"2012-01-01\"\n",
    "start_validation_date = \"2016-01-01\"\n",
    "splitting_date = '2018-01-01'\n",
    "end_date = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters related to the Experience Replay mechanism\n",
    "capacity = 100000\n",
    "batchSize = 32\n",
    "experiencesRequired = 1000\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    GOAL: Implementing the replay memory required for the Experience Replay\n",
    "          mechanism of the DQN Reinforcement Learning algorithm.\n",
    "    \n",
    "    VARIABLES:  - memory: Data structure storing the experiences.\n",
    "                                \n",
    "    METHODS:    - __init__: Initialization of the memory data structure.\n",
    "                - push: Insert a new experience into the replay memory.\n",
    "                - sample: Sample a batch of experiences from the replay memory.\n",
    "                - __len__: Return the length of the replay memory.\n",
    "                - reset: Reset the replay memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=capacity):\n",
    "        \"\"\"\n",
    "        GOAL: Initializating the replay memory data structure.\n",
    "        \n",
    "        INPUTS: - capacity: Capacity of the data structure, specifying the\n",
    "                            maximum number of experiences to be stored\n",
    "                            simultaneously.\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "\n",
    "    def push(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        GOAL: Insert a new experience into the replay memory. An experience\n",
    "              is composed of a state, an action, a reward, a next state and\n",
    "              a termination signal.\n",
    "        \n",
    "        INPUTS: - state: RL state of the experience to be stored.\n",
    "                - action: RL action of the experience to be stored.\n",
    "                - reward: RL reward of the experience to be stored.\n",
    "                - nextState: RL next state of the experience to be stored.\n",
    "                - done: RL termination signal of the experience to be stored.\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        \"\"\"\n",
    "        GOAL: Sample a batch of experiences from the replay memory.\n",
    "        \n",
    "        INPUTS: - batchSize: Size of the batch to sample.\n",
    "        \n",
    "        OUTPUTS: - state: RL states of the experience batch sampled.\n",
    "                 - action: RL actions of the experience batch sampled.\n",
    "                 - reward: RL rewards of the experience batch sampled.\n",
    "                 - nextState: RL next states of the experience batch sampled.\n",
    "                 - done: RL termination signals of the experience batch sampled.\n",
    "        \"\"\"\n",
    "\n",
    "        state, action, reward, nextState, done = zip(*random.sample(self.memory, batchSize))\n",
    "        return state, action, reward, nextState, done\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        GOAL: Return the capicity of the replay memory, which is the maximum number of\n",
    "              experiences which can be simultaneously stored in the replay memory.\n",
    "        \n",
    "        INPUTS: /\n",
    "        \n",
    "        OUTPUTS: - length: Capacity of the replay memory.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        GOAL: Reset (empty) the replay memory.\n",
    "        \n",
    "        INPUTS: /\n",
    "        \n",
    "        OUTPUTS: /\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory = deque(maxlen=capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593d2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters related to the DQN algorithm\n",
    "gamma = 0.4\n",
    "learningRate = 0.0001\n",
    "targetNetworkUpdate = 1000\n",
    "learningUpdatePeriod = 1\n",
    "\n",
    "# Default parameters related to the Deep Neural Network\n",
    "numberOfNeurons = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# Variables defining the default observation and state spaces\n",
    "stateLength = 30\n",
    "observationSpace = 1 + (stateLength-1)*4\n",
    "actionSpace = 2\n",
    "\n",
    "# Default parameter related to the L2 Regularization \n",
    "L2Factor = 0.000001\n",
    "\n",
    "def DQN(numberOfInputs, numberOfOutputs, numberOfNeurons=numberOfNeurons, dropout=dropout):\n",
    "    \"\"\"\n",
    "    GOAL: Implement a Deep Q-Network (DQN) using a functional approach.\n",
    "    \n",
    "    INPUTS:\n",
    "    - numberOfInputs: Number of input features.\n",
    "    - numberOfOutputs: Number of possible actions (Q-values output).\n",
    "    - numberOfNeurons: Number of neurons per hidden layer (default: 128).\n",
    "    - dropout: Dropout probability for regularization (default: 0.1).\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - A PyTorch Sequential model representing the Deep Q-Network.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(numberOfInputs, numberOfNeurons, bias=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "\n",
    "        nn.Linear(numberOfNeurons, numberOfNeurons, bias=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "\n",
    "        nn.Linear(numberOfNeurons, numberOfNeurons, bias=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "\n",
    "        nn.Linear(numberOfNeurons, numberOfNeurons, bias=False),\n",
    "        nn.BatchNorm1d(numberOfNeurons),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        \n",
    "        nn.Linear(numberOfNeurons, numberOfOutputs, bias=False)\n",
    "    )\n",
    "    \n",
    "    # Initialize weights using Xavier initialization\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2344d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "money = 100000\n",
    "\n",
    "def reset_data(data):\n",
    "    \"\"\"\n",
    "    GOAL: Reset the trading environment to its initial state.\n",
    "    \n",
    "    INPUTS: - data: DataFrame containing the trading data.\n",
    "    \n",
    "    OUTPUTS: - data: DataFrame with the reset values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset the trading environment\n",
    "    data['Position'] = 0\n",
    "    data['Action'] = 0\n",
    "    data['Holdings'] = 0.\n",
    "    data['Cash'] = float(money)\n",
    "    data['Money'] = data['Holdings'] + data['Cash']\n",
    "    data['Returns'] = 0.\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906e09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizationCoefficients(data):\n",
    "    \"\"\"\n",
    "    GOAL: Retrieve the coefficients required for the normalization\n",
    "            of input data.\n",
    "    \n",
    "    INPUTS: - tradingEnv: RL trading environement to process.\n",
    "    \n",
    "    OUTPUTS: - coefficients: Normalization coefficients.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the available trading data\n",
    "#     tradingData = tradingEnv.data\n",
    "    closePrices = data['Close'].tolist()\n",
    "    lowPrices = data['Low'].tolist()\n",
    "    highPrices = data['High'].tolist()\n",
    "    volumes = data['Volume'].tolist()\n",
    "\n",
    "    # Retrieve the coefficients required for the normalization\n",
    "    coefficients = []\n",
    "    margin = 1\n",
    "    # 1. Close price => returns (absolute) => maximum value (absolute)\n",
    "    returns = [abs((closePrices[i]-closePrices[i-1])/closePrices[i-1]) for i in range(1, len(closePrices))]\n",
    "    coeffs = (0, np.max(returns)*margin)\n",
    "    coefficients.append(coeffs)\n",
    "    # 2. Low/High prices => Delta prices => maximum value\n",
    "    deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(len(lowPrices))]\n",
    "    coeffs = (0, np.max(deltaPrice)*margin)\n",
    "    coefficients.append(coeffs)\n",
    "    # 3. Close/Low/High prices => Close price position => no normalization required\n",
    "    coeffs = (0, 1)\n",
    "    coefficients.append(coeffs)\n",
    "    # 4. Volumes => minimum and maximum values\n",
    "    coeffs = (np.min(volumes)/margin, np.max(volumes)*margin)\n",
    "    coefficients.append(coeffs)\n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845cdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initState(data, stateLength):\n",
    "    \"\"\"\n",
    "    GOAL: Setting an arbitrary starting point regarding the trading activity.\n",
    "            This technique is used for better generalization of the RL agent.\n",
    "    \n",
    "    INPUTS: - startingPoint: Optional starting point (iteration) of the trading activity.\n",
    "    \n",
    "    OUTPUTS: /\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the RL variables common to every OpenAI gym environments\n",
    "    state = [data['Close'][0: stateLength].tolist(),\n",
    "                    data['Low'][0: stateLength].tolist(),\n",
    "                    data['High'][0: stateLength].tolist(),\n",
    "                    data['Volume'][0: stateLength].tolist(),\n",
    "                    [0]]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afbcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(state, coefficients):\n",
    "    \"\"\"\n",
    "    GOAL: Process the RL state returned by the environment\n",
    "            (appropriate format and normalization).\n",
    "    \n",
    "    INPUTS: - state: RL state returned by the environment.\n",
    "    \n",
    "    OUTPUTS: - state: Processed RL state.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalization of the RL state\n",
    "    closePrices = [state[0][i] for i in range(len(state[0]))]\n",
    "    lowPrices = [state[1][i] for i in range(len(state[1]))]\n",
    "    highPrices = [state[2][i] for i in range(len(state[2]))]\n",
    "    volumes = [state[3][i] for i in range(len(state[3]))]\n",
    "\n",
    "    # 1. Close price => returns => MinMax normalization\n",
    "    returns = [(closePrices[i]-closePrices[i-1])/closePrices[i-1] for i in range(1, len(closePrices))]\n",
    "    if coefficients[0][0] != coefficients[0][1]:\n",
    "        state[0] = [((x - coefficients[0][0])/(coefficients[0][1] - coefficients[0][0])) for x in returns]\n",
    "    else:\n",
    "        state[0] = [0 for x in returns]\n",
    "    # 2. Low/High prices => Delta prices => MinMax normalization\n",
    "    deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(1, len(lowPrices))]\n",
    "    if coefficients[1][0] != coefficients[1][1]:\n",
    "        state[1] = [((x - coefficients[1][0])/(coefficients[1][1] - coefficients[1][0])) for x in deltaPrice]\n",
    "    else:\n",
    "        state[1] = [0 for x in deltaPrice]\n",
    "    # 3. Close/Low/High prices => Close price position => No normalization required\n",
    "    closePricePosition = []\n",
    "    for i in range(1, len(closePrices)):\n",
    "        deltaPrice = abs(highPrices[i]-lowPrices[i])\n",
    "        if deltaPrice != 0:\n",
    "            item = abs(closePrices[i]-lowPrices[i])/deltaPrice\n",
    "        else:\n",
    "            item = 0.5\n",
    "        closePricePosition.append(item)\n",
    "    if coefficients[2][0] != coefficients[2][1]:\n",
    "        state[2] = [((x - coefficients[2][0])/(coefficients[2][1] - coefficients[2][0])) for x in closePricePosition]\n",
    "    else:\n",
    "        state[2] = [0.5 for x in closePricePosition]\n",
    "    # 4. Volumes => MinMax normalization\n",
    "    volumes = [volumes[i] for i in range(1, len(volumes))]\n",
    "    if coefficients[3][0] != coefficients[3][1]:\n",
    "        state[3] = [((x - coefficients[3][0])/(coefficients[3][1] - coefficients[3][0])) for x in volumes]\n",
    "    else:\n",
    "        state[3] = [0 for x in volumes]\n",
    "    \n",
    "    # Process the state structure to obtain the appropriate format\n",
    "    state = [item for sublist in state for item in sublist]\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49dc059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(state, mainNetwork):\n",
    "        \"\"\"\n",
    "        GOAL: Choose a valid RL action from the action space according to the\n",
    "              RL policy as well as the current RL state observed.\n",
    "        \n",
    "        INPUTS: - state: RL state returned by the environment.\n",
    "        \n",
    "        OUTPUTS: - action: RL action chosen from the action space.\n",
    "                 - Q: State-action value function associated.\n",
    "                 - QValues: Array of all the Qvalues outputted by the\n",
    "                            Deep Neural Network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Choose the best action based on the RL policy\n",
    "        with torch.no_grad():\n",
    "            tensorState = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "            QValues = mainNetwork(tensorState).squeeze(0)\n",
    "            Q, action = QValues.max(0)\n",
    "            action = action.item()\n",
    "            Q = Q.item()\n",
    "            QValues = QValues.cpu().numpy()\n",
    "            return action, Q, QValues\n",
    "        \n",
    "# Default parameters related to the Epsilon-Greedy exploration technique\n",
    "epsilonStart = 1.0\n",
    "epsilonEnd = 0.01\n",
    "epsilonDecay = 10000\n",
    "\n",
    "# Default parameters regarding the sticky actions RL generalization technique\n",
    "alpha = 0.1\n",
    "\n",
    "def chooseActionEpsilonGreedy(state, previousAction, mainNetwork, iterations):\n",
    "    \"\"\"\n",
    "    GOAL: Choose a valid RL action from the action space according to the\n",
    "            RL policy as well as the current RL state observed, following the \n",
    "            Epsilon Greedy exploration mechanism.\n",
    "\n",
    "    INPUTS: - state: RL state returned by the environment.\n",
    "            - previousAction: Previous RL action executed by the agent.\n",
    "\n",
    "    OUTPUTS: - action: RL action chosen from the action space.\n",
    "                - Q: State-action value function associated.\n",
    "                - QValues: Array of all the Qvalues outputted by the\n",
    "                        Deep Neural Network.\n",
    "    \"\"\"\n",
    "    epsilonValue = lambda iteration: epsilonEnd + (epsilonStart - epsilonEnd) * math.exp(-1 * iteration / epsilonDecay)\n",
    "    # EXPLOITATION -> RL policy\n",
    "    if(random.random() > epsilonValue(iterations)):\n",
    "        # Sticky action (RL generalization mechanism)\n",
    "        if(random.random() > alpha):\n",
    "            action, Q, QValues = chooseAction(state, mainNetwork)\n",
    "        else:\n",
    "            action = previousAction\n",
    "            Q = 0\n",
    "            QValues = [0, 0]\n",
    "\n",
    "    # EXPLORATION -> Random\n",
    "    else:\n",
    "        action = random.randrange(actionSpace)\n",
    "        Q = 0\n",
    "        QValues = [0, 0]\n",
    "\n",
    "    # Increment the iterations counter (for Epsilon Greedy)\n",
    "    iterations += 1\n",
    "\n",
    "    return action, Q, QValues, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "425953bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLowerBound(cash, numberOfShares, price, transactionCosts, epsilon):\n",
    "    \"\"\"\n",
    "    GOAL: Compute the lower bound of the complete RL action space, \n",
    "            i.e. the minimum number of share to trade.\n",
    "    \n",
    "    INPUTS: - cash: Value of the cash owned by the agent.\n",
    "            - numberOfShares: Number of shares owned by the agent.\n",
    "            - price: Last price observed.\n",
    "    \n",
    "    OUTPUTS: - lowerBound: Lower bound of the RL action space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Computation of the RL action lower bound\n",
    "    deltaValues = - cash - numberOfShares * price * (1 + epsilon) * (1 + transactionCosts)\n",
    "    if deltaValues < 0:\n",
    "        lowerBound = deltaValues / (price * (2 * transactionCosts + (epsilon * (1 + transactionCosts))))\n",
    "    else:\n",
    "        lowerBound = deltaValues / (price * epsilon * (1 + transactionCosts))\n",
    "    return lowerBound\n",
    "\n",
    "def step(action, data, t, numberOfShares, transactionCosts, stateLength, epsilon, done):\n",
    "    \"\"\"\n",
    "    GOAL: Transition to the next trading time step based on the\n",
    "            trading position decision made (either long or short).\n",
    "    \n",
    "    INPUTS: - action: Trading decision (1 = long, 0 = short).    \n",
    "    \n",
    "    OUTPUTS: - state: RL state to be returned to the RL agent.\n",
    "                - reward: RL reward to be returned to the RL agent.\n",
    "                - done: RL episode termination signal (boolean).\n",
    "                - info: Additional information returned to the RL agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stting of some local variables\n",
    "    # t = t\n",
    "    numberOfShares = numberOfShares\n",
    "    customReward = False\n",
    "\n",
    "    # CASE 1: LONG POSITION\n",
    "    if(action == 1):\n",
    "        data['Position'][t] = 1\n",
    "        # Case a: Long -> Long\n",
    "        if(data['Position'][t - 1] == 1):\n",
    "            data['Cash'][t] = data['Cash'][t - 1]\n",
    "            data['Holdings'][t] = numberOfShares * data['Close'][t]\n",
    "        # Case b: No position -> Long\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            data['Holdings'][t] = numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = 1\n",
    "        # Case c: Short -> Long\n",
    "        else:\n",
    "            data['Cash'][t] = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            numberOfShares = math.floor(data['Cash'][t]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            data['Holdings'][t] = numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = 1\n",
    "\n",
    "    # CASE 2: SHORT POSITION\n",
    "    elif(action == 0):\n",
    "        data['Position'][t] = -1\n",
    "        # Case a: Short -> Short\n",
    "        if(data['Position'][t - 1] == -1):\n",
    "            lowerBound = computeLowerBound(data['Cash'][t - 1], -numberOfShares, data['Close'][t-1], transactionCosts, epsilon)\n",
    "            if lowerBound <= 0:\n",
    "                data['Cash'][t] = data['Cash'][t - 1]\n",
    "                data['Holdings'][t] =  - numberOfShares * data['Close'][t]\n",
    "            else:\n",
    "                numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n",
    "                numberOfShares -= numberOfSharesToBuy\n",
    "                data['Cash'][t] = data['Cash'][t - 1] - numberOfSharesToBuy * data['Close'][t] * (1 + transactionCosts)\n",
    "                data['Holdings'][t] =  - numberOfShares * data['Close'][t]\n",
    "                customReward = True\n",
    "        # Case b: No position -> Short\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            data['Holdings'][t] = - numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = -1\n",
    "        # Case c: Long -> Short\n",
    "        else:\n",
    "            data['Cash'][t] = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            numberOfShares = math.floor(data['Cash'][t]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            data['Cash'][t] = data['Cash'][t] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            data['Holdings'][t] = - numberOfShares * data['Close'][t]\n",
    "            data['Action'][t] = -1\n",
    "\n",
    "    # CASE 3: PROHIBITED ACTION\n",
    "    else:\n",
    "        raise SystemExit(\"Prohibited action! Action should be either 1 (long) or 0 (short).\")\n",
    "\n",
    "    # Update the total amount of money owned by the agent, as well as the return generated\n",
    "    data['Money'][t] = data['Holdings'][t] + data['Cash'][t]\n",
    "    data['Returns'][t] = (data['Money'][t] - data['Money'][t-1])/data['Money'][t-1]\n",
    "\n",
    "    # Set the RL reward returned to the trading agent\n",
    "    if not customReward:\n",
    "        reward = data['Returns'][t]\n",
    "    else:\n",
    "        reward = (data['Close'][t-1] - data['Close'][t])/data['Close'][t-1]\n",
    "\n",
    "    # numberOfSharesMain = numberOfShares\n",
    "    # Transition to the next trading time step\n",
    "    t_state = t + 1\n",
    "    state = [data['Close'][t_state - stateLength : t_state].tolist(),\n",
    "                    data['Low'][t_state - stateLength : t_state].tolist(),\n",
    "                    data['High'][t_state - stateLength : t_state].tolist(),\n",
    "                    data['Volume'][t_state - stateLength : t_state].tolist(),\n",
    "                    [data['Position'][t_state - 1]]]\n",
    "    \n",
    "    if(t == data.shape[0]):\n",
    "        done = 1  \n",
    "\n",
    "    # Same reasoning with the other action (exploration trick)\n",
    "    otherAction = int(not bool(action))\n",
    "    customReward = False\n",
    "    if(otherAction == 1):\n",
    "        otherPosition = 1\n",
    "        if(data['Position'][t - 1] == 1):\n",
    "            otherCash = data['Cash'][t - 1]\n",
    "            otherHoldings = numberOfShares * data['Close'][t]\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            otherHoldings = numberOfShares * data['Close'][t]\n",
    "        else:\n",
    "            otherCash = data['Cash'][t - 1] - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            numberOfShares = math.floor(otherCash/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = otherCash - numberOfShares * data['Close'][t] * (1 + transactionCosts)\n",
    "            otherHoldings = numberOfShares * data['Close'][t]\n",
    "    else:\n",
    "        otherPosition = -1\n",
    "        if(data['Position'][t - 1] == -1):\n",
    "            lowerBound = computeLowerBound(data['Cash'][t - 1], -numberOfShares, data['Close'][t-1], transactionCosts, epsilon)\n",
    "            if lowerBound <= 0:\n",
    "                otherCash = data['Cash'][t - 1]\n",
    "                otherHoldings =  - numberOfShares * data['Close'][t]\n",
    "            else:\n",
    "                numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n",
    "                numberOfShares -= numberOfSharesToBuy\n",
    "                otherCash = data['Cash'][t - 1] - numberOfSharesToBuy * data['Close'][t] * (1 + transactionCosts)\n",
    "                otherHoldings =  - numberOfShares * data['Close'][t]\n",
    "                customReward = True\n",
    "        elif(data['Position'][t - 1] == 0):\n",
    "            numberOfShares = math.floor(data['Cash'][t - 1]/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            otherHoldings = - numberOfShares * data['Close'][t]\n",
    "        else:\n",
    "            otherCash = data['Cash'][t - 1] + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            numberOfShares = math.floor(otherCash/(data['Close'][t] * (1 + transactionCosts)))\n",
    "            otherCash = otherCash + numberOfShares * data['Close'][t] * (1 - transactionCosts)\n",
    "            otherHoldings = - numberOfShares * data['Close'][t]\n",
    "    otherMoney = otherHoldings + otherCash\n",
    "    if not customReward:\n",
    "        otherReward = (otherMoney - data['Money'][t-1])/data['Money'][t-1]\n",
    "    else:\n",
    "        otherReward = (data['Close'][t-1] - data['Close'][t])/data['Close'][t-1]\n",
    "    otherState = [data['Close'][t_state - stateLength : t_state].tolist(),\n",
    "                    data['Low'][t_state - stateLength : t_state].tolist(),\n",
    "                    data['High'][t_state - stateLength : t_state].tolist(),\n",
    "                    data['Volume'][t_state - stateLength : t_state].tolist(),\n",
    "                    [otherPosition]]\n",
    "    info = {'State' : otherState, 'Reward' : otherReward, 'Done' : done} #, 'Done' : done}\n",
    "\n",
    "    # Return the trading environment feedback to the RL trading agent\n",
    "    return state, reward, info, numberOfShares, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a13c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardClipping = 1\n",
    "\n",
    "def processReward(reward):\n",
    "    \"\"\"\n",
    "    GOAL: Process the RL reward returned by the environment by clipping\n",
    "            its value. Such technique has been shown to improve the stability\n",
    "            the DQN algorithm.\n",
    "    \n",
    "    INPUTS: - reward: RL reward returned by the environment.\n",
    "    \n",
    "    OUTPUTS: - reward: Process RL reward.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.clip(reward, -rewardClipping, rewardClipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59fed323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetNetwork(iterations, mainNetwork, targetNetwork):\n",
    "    \"\"\"\n",
    "    GOAL: Update the target network weights with the main network weights\n",
    "            every targetNetworkUpdate iterations.\n",
    "    \n",
    "    INPUTS: - iterations: Number of iterations executed.\n",
    "            - mainNetwork: Main DQN network.\n",
    "            - targetNetwork: Target DQN network.\n",
    "    \n",
    "    OUTPUTS: /\n",
    "    \"\"\"\n",
    "\n",
    "    if(iterations % targetNetworkUpdate == 0):\n",
    "        targetNetwork.load_state_dict(mainNetwork.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0c6a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientClipping = 1\n",
    "batchSize = 32  # Size of the batch used for the training of the DQN\n",
    "# Default parameters related to the DQN algorithm\n",
    "gamma = 0.4\n",
    "learningRate = 0.0001\n",
    "targetNetworkUpdate = 1000\n",
    "learningUpdatePeriod = 1\n",
    "\n",
    "GPUNumber = 0\n",
    "device = torch.device('cuda:'+str(GPUNumber) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def learning(iterations, t, T_end, replayMemory, mainNetwork, targetNetwork, optimizer, batchSize=batchSize):\n",
    "    \"\"\"\n",
    "    GOAL: Sample a batch of past experiences and learn from it\n",
    "            by updating the Reinforcement Learning policy.\n",
    "    \n",
    "    INPUTS: batchSize: Size of the batch to sample from the replay memory.\n",
    "    \n",
    "    OUTPUTS: /\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that the replay memory is filled enough\n",
    "    if (len(replayMemory) >= batchSize):\n",
    "        # Set the Deep Neural Network in training mode\n",
    "        mainNetwork.train()\n",
    "\n",
    "        # Sample a batch of experiences from the replay memory\n",
    "        state, action, reward, nextState = replayMemory.sample(batchSize)\n",
    "\n",
    "        # Initialization of Pytorch tensors for the RL experience elements\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float, device=device)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "        nextState = torch.tensor(nextState, dtype=torch.float, device=device)\n",
    "        # done = torch.tensor(done, dtype=torch.float, device=device)\n",
    "\n",
    "        # Compute the current Q values returned by the policy network\n",
    "        currentQValues = mainNetwork(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute the next Q values returned by the target network\n",
    "        with torch.no_grad():\n",
    "            nextActions = torch.max(mainNetwork(nextState), 1)[1]\n",
    "            nextQValues = targetNetwork(nextState).gather(1, nextActions.unsqueeze(1)).squeeze(1)\n",
    "            expectedQValues = reward + gamma * nextQValues * (1 - (t+1 == T_end))\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        loss = F.smooth_l1_loss(currentQValues, expectedQValues)\n",
    "\n",
    "        # Computation of the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(mainNetwork.parameters(), gradientClipping)\n",
    "\n",
    "        # Perform the Deep Neural Network optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # If required, update the target deep neural network (update frequency)\n",
    "        updateTargetNetwork(iterations, mainNetwork, targetNetwork)\n",
    "\n",
    "        # Set back the Deep Neural Network in evaluation mode\n",
    "        mainNetwork.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88028452",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockName, startingDate, splittingDate, endingDate = 'Apple', '2012-01-01', '2018-01-01', '2020-01-01'\n",
    "dataTraining = pd.read_csv('data/AAPL_'+startingDate+'_'+splitting_date+'.csv').set_index('Date')\n",
    "dataValidation = pd.read_csv('data/AAPL_'+start_validation_date+'_'+splitting_date+'.csv').set_index('Date')\n",
    "dataTest = pd.read_csv('data/AAPL_'+splitting_date+'_'+endingDate+'.csv').set_index('Date')\n",
    "dataTraining = reset_data(dataTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89248706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQDN\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning  # <-- Add this line\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "\n",
    "# 1. Initialize - Set the two Deep Neural Networks of the DQN algorithm (policy/main and target)\n",
    "replayMemory = ReplayMemory(capacity=capacity)  # Experience replay memory\n",
    "main_network = DQN(observationSpace, actionSpace, numberOfNeurons, dropout)   # Main DQN (θ), with Xavier init\n",
    "target_network = DQN(observationSpace, actionSpace, numberOfNeurons, dropout) # Target DQN (θ⁻)\n",
    "target_network.load_state_dict(main_network.state_dict())  # Initialize target network with main network\n",
    "main_network.eval()\n",
    "target_network.eval()\n",
    "\n",
    "# Set the Deep Learning optimizer\n",
    "optimizer = optim.Adam(main_network.parameters(), lr=learningRate, weight_decay=L2Factor)  # Adam optimizer\n",
    "\n",
    "# Set the Epsilon-Greedy exploration technique as a function\n",
    "epsilonValue = lambda iteration: epsilonEnd + (epsilonStart - epsilonEnd) * math.exp(-1 * iteration / epsilonDecay)\n",
    "\n",
    "# Initialization of the iterations counter\n",
    "iterations = 0\n",
    "\n",
    "# 2. Training Loop\n",
    "for episode in tqdm(range(50)):\n",
    "    \n",
    "    # Set the initial RL variables\n",
    "    initial_state = initState(dataTraining, stateLength)\n",
    "    coefficients = getNormalizationCoefficients(dataTraining)\n",
    "    state = processState(initial_state, coefficients)\n",
    "    numberOfShares = 0\n",
    "    transactionCosts = 0\n",
    "    previousAction = 0\n",
    "    done = 0\n",
    "    \n",
    "    while done == 0:\n",
    "\n",
    "        # 3. Epsilon-Greedy Policy\n",
    "        action, _, _, iterations  = chooseActionEpsilonGreedy(state, previousAction, main_network, iterations)  # random action\n",
    "        epsilon = epsilonValue(iterations)  # Epsilon value\n",
    "        # Interact with the environment with the chosen action\n",
    "        nextState, reward, info, numberOfShares = step(action, dataTraining, t, numberOfShares, transactionCosts, stateLength, epsilon)\n",
    "        \n",
    "        # Process the RL variables retrieved and insert this new experience into the Experience Replay memory\n",
    "        reward = processReward(reward)\n",
    "        nextState = processState(nextState, coefficients)\n",
    "        replayMemory.push(state, action, reward, nextState)\n",
    "\n",
    "        # Trick for better exploration\n",
    "        otherAction = int(not bool(action))\n",
    "        otherReward = processReward(info['Reward'])\n",
    "        otherNextState = processState(info['State'], coefficients)\n",
    "        # otherDone = info['Done']\n",
    "        replayMemory.push(state, otherAction, otherReward, otherNextState)#, otherDone)\n",
    "\n",
    "        # Execute the DQN learning procedure - Update the policy/main network and the target network\n",
    "        learning(iterations, t, T_train, replayMemory, main_network, target_network, optimizer, batchSize)  # Learning step\n",
    "        \n",
    "        # Update the RL state\n",
    "        state = nextState\n",
    "        previousAction = action\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
